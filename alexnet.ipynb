{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# AlexNet Code\n# Importing Libraries\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import RMSprop\nimport cv2\nimport tensorflow as tf\n\n# Dimensions of our images.\nimg_width, img_height = 256,256\n\n\n# Input and training parameters\ntrain_data_dir = '/kaggle/input/hne/hfolds/f1/train/'\nvalidation_data_dir = '/kaggle/input/hne/hfolds/f1/validation/'\nnb_train_samples = 2274\nnb_validation_samples = 750\nepochs = 50\nbatch_size = 12\nchanDim = -1\n\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)\n\n\n# Create a sequential model\nmodel = Sequential()\n\n# 1st Convolutional Layer\nmodel.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding='valid', activation='relu'))\n# Pooling \nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n# Batch Normalisation \nmodel.add(BatchNormalization())\n\n# 2nd Convolutional Layer\nmodel.add(ZeroPadding2D(padding=(2, 2)))\nmodel.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid', activation='relu'))\n# Pooling\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 3rd Convolutional Layer\nmodel.add(ZeroPadding2D(padding=(1, 1)))\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 4th Convolutional Layer\nmodel.add(ZeroPadding2D(padding=(1, 1)))\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 5th Convolutional Layer\nmodel.add(ZeroPadding2D(padding=(1, 1)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# Pooling\nmodel.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# Passing it to a dense layer\nmodel.add(Flatten())\n# 1st Dense Layer\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nmodel.add(Dropout(0.4))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 2nd Dense Layer\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\n# Add Dropout\nmodel.add(Dropout(0.4))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# Output Layer\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.summary()\n\n# Compile Model with RMSprop optimizer and 1e-3 learning rate\nopt=tf.keras.optimizers.RMSprop(lr=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n  \n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\nvalidation_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary')\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='binary')\n\nhistory=model.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples // batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Network Layers\nfrom keras.utils import plot_model\nplot_model(model, to_file='model_alexnet.png', show_shapes=True, show_layer_names=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport json\n\nwith open('alexnet.json', 'w') as f:\n    json.dump(str(history.history), f)\n    \n# Plotting Accuracy Curve\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['accuracy', 'val_accuracy']].plot()\nplt.title('Training accuracy vs Validation accuracy of AlexNet for HnE')\nplt.ylabel('Accuracy')\nplt.xlabel('No of epochs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Loss Curve \nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nplt.title('Training loss vs Validation loss of AlexNet for HnE')\nplt.ylabel('Loss')\nplt.xlabel('No of epochs')\nplt.savefig('1bFold1.eps')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing\ntest_data_dir = '/kaggle/input/hne/hfolds/f1/test'\ntest_data_generator = ImageDataGenerator(rescale=1./255)\ntest_generator = test_data_generator.flow_from_directory(\n    test_data_dir,\n    target_size=(256, 256),\n    batch_size=1,\n    class_mode=\"binary\", \n    shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving Model weights\nMODEL_FILE = \"alexnet.h5\"\nmodel.save_weights(MODEL_FILE)\n\n# Getting predictions for test images\nprobabilities = model.predict_generator(test_generator, 762)\nprint(probabilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thresholding prediction probabilities at 0.5\nprob=[]\nprint(len(probabilities))\nfor i in range(0, 762):\n    if (probabilities[i][0] > 0.5):\n        prob.append(1)\n    else:\n        prob.append(0)\n    \nprint(prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating true labels\nlabels=[]\n# Benign\nfor i in range(0, 516):\n    labels.append(0)\n    \n# Malignant\nfor i in range(0, 246):\n    labels.append(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Print the confusion Matrix\ncm = confusion_matrix(labels, prob)\nprint(cm)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}